{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2f49c8827443>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch import optim\n",
    "from random import randint \n",
    "\n",
    "### para la visualización de los objetos 3D\n",
    "# TODO: Leer la función y modificarla para imprimir representación sobre GT, (considerar modificar el alpha)\n",
    "#       Conocer la ubicación de los umbrales para futuras modificaciones\n",
    "\n",
    "#!curl https://raw.githubusercontent.com/irvingvasquez/nbv-net/master/classification_nbv.py > classification_nbv.py\n",
    "#sys.path.append('/kaggle/input/nbvsss')\n",
    "import classification_nbv as cnbv\n",
    "\n",
    "from utils import net_sample_output_nbv, valida_nbv, entrena_nbv, initialize_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Acceso a los datasets\n",
    "#sys.path.append('/kaggle/input')\n",
    "#dataset_folder = '/kaggle/input/'\n",
    "#print(os.listdir('/kaggle/input/interpolated-dataset/archive/training/'))\n",
    "\n",
    "dataset_folder = 'archive/'\n",
    "\n",
    "#file_lbl = 'interpolated-dataset/archive/training/dataset_pose_training.npy'\n",
    "file_lbl = 'validation/dataset_pose_validation.npy'\n",
    "# batch size\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.36613   , -0.0234427 ,  0.159373  , -1.41145649, -1.15631205,\n",
       "        2.9955435 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_input_pose = os.path.join(dataset_folder, file_lbl)\n",
    "dataset_pose = np.load(path_input_pose)\n",
    "dataset_pose[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data size: \n",
      " (3034, 29791)\n"
     ]
    }
   ],
   "source": [
    "# Dirección\n",
    "file_vol = 'validation/dataset_vol_validation_resized_pytorch.npy'\n",
    "\n",
    "# Lectura de entradas\n",
    "path_input_vol = os.path.join(dataset_folder, file_vol)\n",
    "dataset_vol = np.load(path_input_vol)\n",
    "\n",
    "print(\"Input data size: \\n\",dataset_vol.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input validation data size: \n",
      " (3034, 29791)\n"
     ]
    }
   ],
   "source": [
    "dataset_folder_val = 'archive/validation'\n",
    "# address\n",
    "file_vol_val = 'dataset_vol_validation_resized_pytorch.npy'\n",
    "\n",
    "# load the inputs\n",
    "path_input_vol = os.path.join(dataset_folder_val, file_vol_val)\n",
    "dataset_vol = np.load(path_input_vol)\n",
    "\n",
    "print(\"Input validation data size: \\n\",dataset_vol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los datos\n",
    "training_dataset = cnbv.NBVClassificationDatasetFull(grid_file= os.path.join(dataset_folder, file_vol), \n",
    "                                    nbv_class_file=os.path.join(dataset_folder, file_lbl),\n",
    "                                    transform=transforms.Compose([\n",
    "                                    # Reshapes the plain grid\n",
    "                                    cnbv.ToSO3(),\n",
    "                                    cnbv.To3DGrid(),\n",
    "                                    #converts to tensors\n",
    "                                    cnbv.ToTensor()\n",
    "                                   ]))\n",
    "                                                     \n",
    "validation_dataset = cnbv.NBVClassificationDatasetFull(grid_file= os.path.join(dataset_folder_val, file_vol_val), \n",
    "                                    nbv_class_file = os.path.join(dataset_folder_val, 'dataset_pose_validation.npy'),\n",
    "                                    transform=transforms.Compose([\n",
    "                                    # Reshapes the plain grid\n",
    "                                    cnbv.To3DGrid(),\n",
    "                                    cnbv.ToSO3(),\n",
    "                                    #converts to tensors\n",
    "                                    cnbv.ToTensor()\n",
    "                                    ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de los datos en batch\n",
    "train_loader = DataLoader(training_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, \n",
    "                          num_workers=0)\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, \n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Esta el GPU disponible?: True\n"
     ]
    }
   ],
   "source": [
    "print(\"¿Esta el GPU disponible?:\",torch.cuda.is_available())\n",
    "device = torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Definir un modelo de MLP 16x2048x512x512x12 basado en cotinuity of rotations\"\"\"\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,input = 16, output = 12):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self.mlp = nn.Sequential(nn.Linear(self.input,2048),\n",
    "                                 nn.LeakyReLU(),\n",
    "                                 nn.Linear(2048,512),\n",
    "                                 nn.LeakyReLU(),\n",
    "                                 nn.Linear(512,512),\n",
    "                                 nn.LeakyReLU(),\n",
    "                                 nn.Linear(512, self.output),\n",
    "                                 nn.LeakyReLU())\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import linalg as LA\n",
    "def weighted_loss(output, ground_truth, alpha = 0.5):\n",
    "   '''a-> Traslation error\n",
    "    b-> Rotation error'''\n",
    "   alpha = alpha\n",
    "   max = 0.4 # 0.396598\n",
    "   \n",
    "   #Divide y arregla dimensiones\n",
    "   l =len(output)\n",
    "   s = ground_truth[:,:3]\n",
    "   r = ground_truth[:,3:].reshape([l,3,3])\n",
    "   s_1 = output[:,:3]\n",
    "   r_1 = output[:,3:].reshape([l,3,3])\n",
    "   \n",
    "   a = LA.norm((s-s_1), dim=-1, ord=2) / max\n",
    "   b = LA.norm((r-r_1), dim=(-2,-1), ord=2)\n",
    "   loss = alpha*a + (1-alpha)*b\n",
    "   return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenemos una esfera de radio no cambiante, es decir, que estamos condicionando el problema a una esfera de radio constante.\n",
    "Es decir, si conozco la rotación puedo moverme en esa superficie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/saulo/datos/Saulo/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:605: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss min: -5620.5516357421875\n",
      "Numero de epoca: 1 Funcion de perdida(entrenamiento): -904.9073\n",
      "Final\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs=1#Numero de epocas\n",
    "\n",
    "losses = {\"entrenamiento\":[], \"validación\":[]}\n",
    "\n",
    "\n",
    "loss_ant = 10.0 # variable para almacenar el  menor valor de error durante el entrenamiento\n",
    "torch.manual_seed(5068) # inicialización de semilla\n",
    "model= MLP().cuda() # Modelo\n",
    "#Inicialización de pesos con la técnica de Kaiming normal\n",
    "initialize_weights(model, init = \"kaiming_normal\", a =None, b= None, mean= None, std= None, const= None, groups= None, gain= None)\n",
    "criterion=weighted_loss #Metrica error\n",
    "optimizer=torch.optim.Adam(  #Optimizador\n",
    "    model.parameters(), \n",
    "    lr=1e-3, \n",
    "    weight_decay=1e-4)\n",
    "# ciclo en el número de épocas\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print(f\"Época {epoch+1}\\n-------------------------------\")\n",
    "    loss_entrena = entrena_nbv(train_loader, model, criterion, optimizer, device) #Entrenamieno\n",
    "    loss_valida = valida_nbv(validation_loader, model, criterion, device)         #Validación\n",
    "    #============saveb_weights==============\n",
    "    losses[\"entrenamiento\"].append(loss_entrena.item())\n",
    "    losses[\"validación\"].append(loss_valida)\n",
    "\n",
    "    if loss_valida < loss_ant: # si el error en la época actual es menor que el error anterior(el error), almacenas los pesos.\n",
    "        torch.save(model.state_dict(), 'weights_entrenamiento_MLP.pth')\n",
    "        loss_ant = loss_valida\n",
    "        print(\"loss min: {}\".format(loss_ant))\n",
    "\n",
    "    #================log========================    \n",
    "    if epoch % 100 == 0:\n",
    "        loss_value=loss_entrena.cpu().detach().numpy()\n",
    "        #plotea(train_loader, model)\n",
    "        print(\"Numero de epoca:\",epoch+1,'Funcion de perdida(entrenamiento):',loss_value)\n",
    "\n",
    "print('Final\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Acceso a los datasets\n",
    "#sys.path.append('/kaggle/input')\n",
    "#dataset_folder = '/kaggle/input/'\n",
    "#print(os.listdir('/kaggle/input/interpolated-dataset/archive/training/'))\n",
    "\n",
    "dataset_folder = 'archive/'\n",
    "\n",
    "#file_lbl = 'interpolated-dataset/archive/training/dataset_pose_training.npy'\n",
    "file_lbl = 'validation/dataset_pose_validation.npy'\n",
    "# batch size\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.36613   , -0.0234427 ,  0.159373  , -1.41145649, -1.15631205,\n",
       "        2.9955435 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_input_pose = os.path.join(dataset_folder, file_lbl)\n",
    "dataset_pose = np.load(path_input_pose)\n",
    "dataset_pose[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data size: \n",
      " (3034, 29791)\n"
     ]
    }
   ],
   "source": [
    "# Dirección\n",
    "file_vol = 'validation/dataset_vol_validation_resized_pytorch.npy'\n",
    "\n",
    "# Lectura de entradas\n",
    "path_input_vol = os.path.join(dataset_folder, file_vol)\n",
    "dataset_vol = np.load(path_input_vol)\n",
    "\n",
    "print(\"Input data size: \\n\",dataset_vol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input validation data size: \n",
      " (3034, 29791)\n"
     ]
    }
   ],
   "source": [
    "dataset_folder_val = 'archive/validation'\n",
    "# address\n",
    "file_vol_val = 'dataset_vol_validation_resized_pytorch.npy'\n",
    "\n",
    "# load the inputs\n",
    "path_input_vol = os.path.join(dataset_folder_val, file_vol_val)\n",
    "dataset_vol = np.load(path_input_vol)\n",
    "\n",
    "print(\"Input validation data size: \\n\",dataset_vol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los datos                                                   \n",
    "validation_dataset = cnbv.NBVClassificationDatasetFull(grid_file= os.path.join(dataset_folder_val, file_vol_val), \n",
    "                                    nbv_class_file = os.path.join(dataset_folder_val, 'dataset_pose_validation.npy'),\n",
    "                                    transform=transforms.Compose([\n",
    "                                    # Reshapes the plain grid\n",
    "                                    cnbv.To3DGrid(),\n",
    "                                    cnbv.ToSO3(),\n",
    "                                    #converts to tensors\n",
    "                                    cnbv.ToTensor()\n",
    "                                    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de los datos en batch\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False, \n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Esta el GPU disponible?: True\n"
     ]
    }
   ],
   "source": [
    "print(\"¿Esta el GPU disponible?:\",torch.cuda.is_available())\n",
    "device = torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Definir un modelo de MLP 16x2048x512x512x12 basado en cotinuity of rotations\"\"\"\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,input = 16, output = 12):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self.mlp = nn.Sequential(nn.Linear(self.input,2048),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(2048,512),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(512,512),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(512, self.output),\n",
    "                                 nn.Tanh())\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/saulo/datos/Saulo/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:605: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv3d(\n"
     ]
    }
   ],
   "source": [
    "model= MLP().cuda() \n",
    "for i in [\"xavier_normal\", \"kaiming_normal\"]:\n",
    "    for j in [1,2,3,4]:\n",
    "        path_weights = 'stuff/experimento_MLP/weights_entrenamiento_MLP_{}_{}.pth'.format(i,j)\n",
    "        model.load_state_dict(torch.load(path_weights))\n",
    "        output, nbv= net_sample_output_nbv(model, validation_loader, device)\n",
    "        np.save(\"stuff/experimento_MLP/preds/MLP_{}_{}.npy\".format(i,j),output.detach().numpy(), allow_pickle=True)\n",
    "        del output\n",
    "        torch.cuda.empty_cache()\n",
    "np.save(\"stuff/experimento_MLP/preds/MLP_gt.npy\".format(i,j),nbv.detach().numpy(), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from wloss import weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El promedio de error con la suma ponderada es: -0.5004704594612122 de traslación es: 0.7488805651664734 y de rotación es: 97.06854248046875\n",
      "\n",
      "El promedio de error con la suma ponderada es: -0.5018340349197388 de traslación es: 0.7380900382995605 y de rotación es: 96.67076110839844\n",
      "\n",
      "El promedio de error con la suma ponderada es: -0.4622870981693268 de traslación es: 0.7749289274215698 y de rotación es: 94.12692260742188\n",
      "\n",
      "El promedio de error con la suma ponderada es: -0.46553629636764526 de traslación es: 0.7706781029701233 y de rotación es: 94.42802429199219\n",
      "\n",
      "El promedio de error con la suma ponderada es: -0.5264101028442383 de traslación es: 0.7117819786071777 y de rotación es: 98.52960968017578\n",
      "\n",
      "El promedio de error con la suma ponderada es: -0.5472909808158875 de traslación es: 0.6906380653381348 y de rotación es: 100.11519622802734\n",
      "\n",
      "El promedio de error con la suma ponderada es: -0.502632737159729 de traslación es: 0.7292459607124329 y de rotación es: 96.28189849853516\n",
      "\n",
      "El promedio de error con la suma ponderada es: -0.48351001739501953 de traslación es: 0.7344663143157959 y de rotación es: 94.56929016113281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gt = torch.from_numpy(np.load('stuff/experimento_MLP/preds/MLP_gt.npy'))\n",
    "for i in [\"xavier_normal\", \"kaiming_normal\"]:\n",
    "    for j in [1,2,3,4]:\n",
    "        output = np.load('stuff/experimento_MLP/preds/MLP_{}_{}.npy'.format(i,j))\n",
    "        output_t = torch.from_numpy(output)\n",
    "        suma,dif_traslacion,dif_rotacion = weighted_loss(output_t,gt)\n",
    "        #np.savez('stuff/experimento_MLP/preds/diferencias_rotaciones_{}_{}.npz'.format(i,j), suma_ponderada = suma, traslacion = dif_traslacion, rotacion = dif_rotacion)\n",
    "        print(\"El promedio de error con la suma ponderada es: {} de traslación es: {} y de rotación es: {}\\n\".format(suma,torch.nanmean(dif_traslacion),torch.nanmean(dif_rotacion*180/np.pi)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([238, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(torch.isnan(dif_rotacion.view(-1))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.2573e-01,  8.5065e-01,  7.0377e-17, -6.4384e-17,  4.2942e-17,\n",
       "        -1.0000e+00, -8.5065e-01, -5.2573e-01,  3.2192e-17])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt[0,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2028,  0.1101, -0.0536,  0.0370,  0.0572, -0.9722, -0.0321,  0.1899,\n",
       "        -0.0336])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t[0,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_biprimo = gt[0,3:].reshape([3,3])*torch.linalg.inv(output_t[0,3:].reshape([3,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3923e+00, -1.3742e-01, -1.8230e-16],\n",
       "        [-5.2002e-17, -9.1291e-18, -4.8622e+00],\n",
       "        [-1.8779e-01,  5.5055e-01,  6.0262e-18]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_biprimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_error = (M_biprimo[0,0]+M_biprimo[1,1]+M_biprimo[2,2]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.3923)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0944)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arccos(torch.tensor(-.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9855"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.9855+1*2-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8729)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dif_traslacion.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   nan,    nan, 1.9855, 1.1825, 1.5351,    nan, 1.6849, 0.8658, 1.5997,\n",
       "        2.0868, 1.6266, 2.0027,    nan, 1.5923,    nan, 2.2434, 2.0404,    nan,\n",
       "        1.7409, 2.2349,    nan,    nan, 1.2775, 0.9618, 1.1289, 1.8349, 2.0823,\n",
       "        2.1541, 1.9845,    nan, 1.9936, 1.8484, 1.0427,    nan, 1.6846,    nan,\n",
       "        1.3276, 1.2346, 2.7687, 1.9244, 2.3117, 2.7389, 2.1193, 1.8519, 1.3239,\n",
       "        2.1015,    nan, 2.0722, 0.9698, 1.1932, 1.9555, 1.0977, 2.0288,    nan,\n",
       "           nan, 1.8160, 1.4122, 2.0729, 1.5193, 1.1910, 1.5861, 1.2319,    nan,\n",
       "        1.9685, 2.7848, 2.0770, 1.8136, 1.7212, 1.8927, 1.0650,    nan, 1.0016,\n",
       "        1.0952, 1.6330,    nan, 1.7987, 2.5167,    nan, 1.1990, 1.8820, 1.8761,\n",
       "        2.6713, 1.6947, 1.7383, 1.4352, 2.0193, 1.7250,    nan, 1.9571, 1.1294,\n",
       "        1.7687, 2.9575, 1.8076, 1.2474, 1.9886,    nan, 1.8275,    nan,    nan,\n",
       "           nan,    nan,    nan, 0.9740, 2.0312, 1.4973, 0.1502, 0.4292, 1.9171,\n",
       "        0.6852, 1.6590, 1.6479, 2.0803,    nan, 1.5327, 1.2935, 1.1939,    nan,\n",
       "        1.6742, 1.5056, 1.3112, 1.8972, 2.4717, 1.0899, 1.9339, 2.3944, 3.0262,\n",
       "        1.0786, 1.0791,    nan, 2.0638, 1.3047, 2.0871, 1.4258,    nan,    nan,\n",
       "        2.1040,    nan, 1.9709,    nan, 2.1411, 0.5257, 2.1219,    nan, 1.4302,\n",
       "           nan, 0.6175, 0.8816, 1.3327, 1.2583, 1.8854, 1.9631, 2.1484, 1.8667,\n",
       "        1.5385,    nan, 1.5532, 1.0527, 1.7753, 2.1330, 0.9077, 0.9391, 1.0616,\n",
       "           nan, 1.3840,    nan, 0.9844, 1.2535, 1.5581, 1.8579,    nan, 1.2367,\n",
       "           nan, 1.7131, 1.0572, 1.7746, 2.6950,    nan, 1.6941,    nan, 0.6723,\n",
       "        1.1888, 1.7156,    nan, 1.8281,    nan, 1.8151, 1.3049, 1.9300, 1.1747,\n",
       "        1.2808, 1.9910, 1.2898, 1.1055, 0.9776, 1.8792,    nan,    nan, 1.4144,\n",
       "        2.2216,    nan,    nan, 1.9043,    nan, 1.1952, 2.6551, 1.5028, 1.5738,\n",
       "           nan, 2.4602, 1.0572,    nan,    nan, 1.5493, 1.2268, 1.3203, 1.6461,\n",
       "        1.8732, 1.5262, 1.3612,    nan, 2.8965,    nan,    nan, 1.5991, 1.8596,\n",
       "        1.3582, 1.6195, 1.5381, 1.4492, 2.1717, 2.0472, 1.2614, 1.4916, 1.2625,\n",
       "           nan, 1.5448, 1.7010, 1.3070, 1.8823, 2.1978,    nan,    nan, 1.7312,\n",
       "        1.2619,    nan, 1.2275, 1.9262,    nan,    nan, 1.9804,    nan, 0.8951,\n",
       "        2.9291, 2.5123, 0.9451,    nan,    nan, 2.5952,    nan, 1.8078, 0.9650,\n",
       "        1.8241, 2.1165,    nan, 2.1204, 1.5730, 1.4575, 1.4676, 2.2251, 1.3086,\n",
       "        1.5474, 1.9314,    nan, 1.4537,    nan,    nan, 1.3453,    nan,    nan,\n",
       "        1.8716, 1.9532,    nan, 1.7441,    nan,    nan, 1.6691, 2.1770,    nan,\n",
       "        0.4262,    nan, 1.3494, 1.3701, 1.1066,    nan, 2.0613,    nan, 1.3255,\n",
       "        0.6536, 1.3125,    nan, 1.5462, 1.8416,    nan,    nan, 2.4762, 1.6296,\n",
       "        1.6093,    nan,    nan,    nan, 1.0291, 1.0842, 1.1241, 2.0586, 2.5304,\n",
       "        1.5900, 1.3973, 1.9737, 1.5531, 0.9265, 1.6177, 0.3591, 1.6098, 2.4377,\n",
       "        1.9466, 1.7465, 1.9352, 2.0674, 1.7546, 2.1130, 1.1583,    nan, 2.0229,\n",
       "        1.5576,    nan, 0.8672, 2.2235, 2.6130, 2.0159, 1.6287, 1.2335,    nan,\n",
       "        1.7092, 1.0592, 2.0463, 1.3832,    nan, 1.9993, 1.6807, 2.4274,    nan,\n",
       "           nan, 1.5711,    nan, 1.5297,    nan, 1.5272, 1.7302, 1.5730, 2.0159,\n",
       "        1.7188, 2.5522, 1.7797,    nan, 1.8208, 0.8030,    nan, 1.5753, 1.9136,\n",
       "        1.2559, 1.8498, 0.8305, 2.2576, 1.2184, 1.5392, 1.9396, 1.5342, 1.9207,\n",
       "        2.0801, 1.6844, 1.4409,    nan, 1.1882, 1.8388,    nan, 1.1863, 2.1514,\n",
       "        1.9937,    nan,    nan,    nan, 1.9684, 1.2779, 1.6449,    nan, 1.4025,\n",
       "           nan,    nan, 1.6625, 2.0622,    nan, 2.0740, 2.1081, 2.1440, 1.5181,\n",
       "        1.9251, 1.7703,    nan,    nan, 1.3228, 1.1657, 1.9117,    nan, 1.8914,\n",
       "           nan, 2.2048,    nan, 1.9054,    nan, 1.7588,    nan, 1.9489,    nan,\n",
       "        1.2844, 1.5598, 0.9735, 2.1419, 3.0608, 1.8805, 1.8101, 1.7039, 0.9293,\n",
       "        1.6854,    nan, 1.4526, 0.5746,    nan, 1.4373,    nan, 2.1871, 1.9489,\n",
       "        1.5224, 1.9804,    nan, 1.9723, 0.1283, 2.1051, 1.1235, 0.8607, 1.9026,\n",
       "        2.2999, 2.2349, 1.3388, 1.4981, 1.9509, 1.4309, 2.1909, 2.0764, 1.5500,\n",
       "        2.0119, 2.0270, 1.3377,    nan,    nan, 1.8272,    nan, 1.5621, 2.1127,\n",
       "        1.4720, 2.3631, 1.9800, 1.0796, 1.8088, 2.7602, 2.6399, 0.7343,    nan,\n",
       "        0.6361, 1.5040, 1.1587, 1.1423, 1.3894, 2.5935,    nan, 0.5340, 1.3701,\n",
       "        0.5218, 1.5707, 1.7640,    nan, 0.9101, 1.4046, 2.1690, 2.1323, 1.2882,\n",
       "           nan, 0.8851, 1.7975, 1.9154, 1.7932,    nan,    nan, 1.0361, 1.6670,\n",
       "        1.5516,    nan, 2.4375,    nan, 2.0569, 1.2537, 2.2194, 1.9759,    nan,\n",
       "        1.3085,    nan, 2.4642, 1.8943, 2.1616, 1.4856, 1.4173, 1.7586,    nan,\n",
       "        1.6229,    nan, 1.5308, 1.1722, 0.7914, 1.3867, 1.1835, 1.7805, 1.6794,\n",
       "        1.4165, 1.2505, 2.0651, 0.8886, 2.1583, 1.0106, 2.0997, 1.0526,    nan,\n",
       "        1.8373,    nan,    nan,    nan, 1.2359,    nan, 0.9183, 2.3566, 1.7244,\n",
       "           nan,    nan, 2.3455, 0.9149,    nan,    nan, 2.2124, 1.9242, 1.4503,\n",
       "        1.4530, 1.9117, 1.4668,    nan, 1.4622, 1.3038, 2.2321, 1.4662, 2.1205,\n",
       "        1.4664, 1.6937, 1.0598,    nan, 1.5529, 1.3340, 1.5215, 1.5273,    nan,\n",
       "        1.4459,    nan,    nan, 2.0619, 1.6746, 2.4465, 2.0563, 1.0183,    nan,\n",
       "           nan,    nan, 1.2962,    nan, 2.3396, 2.0598,    nan, 1.2627, 0.9982,\n",
       "           nan, 1.2882, 2.3119, 2.6953, 1.8570,    nan, 1.8645, 2.5830,    nan,\n",
       "        1.7527,    nan, 1.9237, 0.9954, 2.0419,    nan, 1.6387, 1.7435, 1.7954,\n",
       "        1.7554, 2.5247, 2.0361,    nan, 0.5995, 1.9170, 2.0955, 1.8721, 2.2699,\n",
       "           nan, 1.4683, 1.8788,    nan,    nan, 2.0087,    nan, 1.1120, 2.0711,\n",
       "        2.2301, 1.7555,    nan, 1.5070, 1.9986, 0.7370, 1.6849,    nan, 2.6145,\n",
       "        1.8354, 2.5977,    nan, 1.5813, 2.0720, 2.4048, 1.1400, 1.3518, 2.4167,\n",
       "        2.0096, 0.9077, 1.9327, 2.0985, 2.3283,    nan, 0.4186, 1.5504,    nan,\n",
       "        2.0480,    nan, 2.4412,    nan, 1.5490,    nan, 1.4269, 1.8574, 1.5954,\n",
       "           nan, 1.7496, 1.2249, 2.0738,    nan, 1.0911, 2.2247,    nan, 2.0540,\n",
       "        1.5569, 1.1314, 1.8979,    nan, 1.6753,    nan,    nan, 1.9210, 1.4683,\n",
       "        1.7981,    nan, 1.6729, 1.4892,    nan, 2.6881, 1.2934, 1.2110, 1.3267,\n",
       "        1.6694,    nan, 1.9262, 1.5546,    nan, 1.7226, 1.5609, 1.3116,    nan,\n",
       "        1.0418,    nan, 2.2987,    nan, 1.2774, 2.1594,    nan, 1.2167, 0.6478,\n",
       "           nan, 1.9039, 2.3937, 1.0508, 1.6389, 0.9773, 2.3221, 1.2007, 1.3560,\n",
       "        1.5650, 2.7570, 1.8262, 2.0669, 1.1458, 1.5467,    nan, 1.0168, 0.9980,\n",
       "        1.2557,    nan, 1.6546,    nan, 1.5261,    nan, 1.5744, 2.3776, 1.5023,\n",
       "        2.1215, 2.0639, 1.9709, 2.3680, 1.9171, 2.0231, 1.4205, 0.9592, 1.8612,\n",
       "        1.1020, 1.4481, 1.5174, 1.1472, 2.0741, 2.1592,    nan, 1.2167, 1.8241,\n",
       "           nan, 0.7363, 1.6801, 1.8252, 1.3445,    nan,    nan, 0.9217,    nan,\n",
       "        2.0808,    nan, 1.5383, 1.6616, 1.9259, 2.5925, 1.6221, 2.2857, 1.7984,\n",
       "        2.0130, 1.8766, 2.9043, 1.6337, 1.8344, 1.0989, 2.0056, 2.4715, 1.3957,\n",
       "        0.8276, 1.7264, 1.3475,    nan, 1.9002, 1.4432,    nan, 1.2453, 1.1838,\n",
       "        2.1016, 1.5459, 1.4346,    nan, 0.7883,    nan, 2.2920, 1.5643, 2.0870,\n",
       "        1.2786, 1.1867, 1.8037, 1.9527, 1.5802, 2.2698, 2.0675, 0.7963, 1.5201,\n",
       "        1.1697, 2.0823, 2.1501, 1.0089, 1.6923,    nan, 0.8070,    nan, 1.0510,\n",
       "        1.3704, 2.4682, 1.7536, 2.0002, 1.4886,    nan, 0.9532, 2.1354, 1.5369,\n",
       "           nan, 1.9322, 2.0248, 1.0093, 0.9384, 2.0499, 0.4394,    nan, 1.9206,\n",
       "        2.2070, 1.6764, 1.3724, 1.6535, 1.2248, 2.1897,    nan, 1.5111, 1.3737,\n",
       "        1.5823,    nan, 2.1542, 1.5866, 2.2046, 1.0782, 2.0042, 0.9900,    nan,\n",
       "           nan, 1.9497,    nan, 2.3576, 1.7807,    nan, 2.3945, 1.3086, 1.7584,\n",
       "           nan, 2.8930, 1.8917, 2.1095, 0.5354, 1.8403, 1.0975, 1.5938, 1.6448,\n",
       "        1.1286, 2.9391, 1.9234, 1.4384,    nan, 0.7734,    nan, 0.2930, 1.4382,\n",
       "        1.6109,    nan, 1.2729, 2.1415, 1.2116, 1.6083, 0.8753,    nan, 1.7682,\n",
       "        1.3118, 1.9355, 0.9619, 1.3354, 1.8520, 0.8262, 1.3764, 1.6252, 1.2360,\n",
       "        1.9246, 0.8312, 2.0731, 1.9089,    nan, 2.0525, 2.0058, 1.7157, 2.4836,\n",
       "        1.5324, 2.0276, 1.8324, 1.6413, 1.9822, 2.1035, 1.2983,    nan, 1.7909,\n",
       "        1.1832,    nan, 1.3863, 1.0775, 0.7835, 2.0478, 1.5164,    nan, 0.9557,\n",
       "        1.5905,    nan, 1.5471,    nan, 1.8633, 2.0511, 1.0822, 1.9772,    nan,\n",
       "        0.8440,    nan, 1.7991,    nan, 1.7231, 2.0773, 1.9393, 2.0528, 1.6614,\n",
       "           nan, 1.3460, 1.9195, 1.7064,    nan, 1.2571, 1.7465, 1.3326,    nan,\n",
       "        1.1646, 0.6032,    nan, 1.7258, 0.9214,    nan, 1.7358,    nan,    nan,\n",
       "        1.7995, 1.8926, 1.8881, 1.2314, 0.4536, 0.7338,    nan,    nan, 1.8690,\n",
       "           nan, 1.3734, 2.0553,    nan,    nan, 1.4386,    nan, 0.9658, 2.0674,\n",
       "           nan, 1.8636, 1.4998, 2.1446,    nan, 1.9480,    nan, 1.6090, 1.6489,\n",
       "        1.1046, 1.5976, 0.9660, 2.9420, 1.1835, 1.3183, 2.1577, 1.5469,    nan,\n",
       "           nan])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dif_rotacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     nan,      nan, 113.7581,  67.7534,  87.9574,      nan,  96.5356,\n",
       "         49.6054,  91.6569, 119.5670,  93.1966, 114.7472,      nan,  91.2317,\n",
       "             nan, 128.5368, 116.9089,      nan,  99.7445, 128.0478,      nan,\n",
       "             nan,  73.1933,  55.1062,  64.6789, 105.1331, 119.3066, 123.4233,\n",
       "        113.7058,      nan, 114.2222, 105.9027,  59.7415,      nan,  96.5179,\n",
       "             nan,  76.0669,  70.7353, 158.6364, 110.2613, 132.4500, 156.9274,\n",
       "        121.4257, 106.1061,  75.8529, 120.4056,      nan, 118.7304,  55.5627,\n",
       "         68.3648, 112.0424,  62.8928, 116.2398,      nan,      nan, 104.0471,\n",
       "         80.9113, 118.7672,  87.0493,  68.2372,  90.8746,  70.5836,      nan,\n",
       "        112.7893, 159.5593, 119.0011, 103.9122,  98.6167, 108.4417,  61.0208,\n",
       "             nan,  57.3849,  62.7486,  93.5651,      nan, 103.0580, 144.1972,\n",
       "             nan,  68.6963, 107.8300, 107.4953, 153.0553,  97.1018,  99.5971,\n",
       "         82.2315, 115.6954,  98.8359,      nan, 112.1337,  64.7071, 101.3380,\n",
       "        169.4515, 103.5692,  71.4726, 113.9387,      nan, 104.7065,      nan,\n",
       "             nan,      nan,      nan,      nan,  55.8073, 116.3809,  85.7880,\n",
       "          8.6087,  24.5910, 109.8402,  39.2613,  95.0550,  94.4179, 119.1908,\n",
       "             nan,  87.8171,  74.1126,  68.4074,      nan,  95.9254,  86.2631,\n",
       "         75.1235, 108.6993, 141.6164,  62.4481, 110.8046, 137.1870, 173.3908,\n",
       "         61.8015,  61.8299,      nan, 118.2447,  74.7556, 119.5843,  81.6933,\n",
       "             nan,      nan, 120.5509,      nan, 112.9242,      nan, 122.6777,\n",
       "         30.1186, 121.5763,      nan,  81.9452,      nan,  35.3798,  50.5125,\n",
       "         76.3586,  72.0967, 108.0260, 112.4800, 123.0924, 106.9562,  88.1498,\n",
       "             nan,  88.9910,  60.3180, 101.7194, 122.2137,  52.0063,  53.8066,\n",
       "         60.8248,      nan,  79.2981,      nan,  56.3994,  71.8198,  89.2720,\n",
       "        106.4483,      nan,  70.8562,      nan,  98.1543,  60.5712, 101.6777,\n",
       "        154.4105,      nan,  97.0637,      nan,  38.5222,  68.1112,  98.2943,\n",
       "             nan, 104.7397,      nan, 103.9964,  74.7642, 110.5784,  67.3057,\n",
       "         73.3821, 114.0764,  73.8988,  63.3433,  56.0125, 107.6682,      nan,\n",
       "             nan,  81.0382, 127.2897,      nan,      nan, 109.1073,      nan,\n",
       "         68.4818, 152.1267,  86.1041,  90.1713,      nan, 140.9563,  60.5743,\n",
       "             nan,      nan,  88.7662,  70.2880,  75.6497,  94.3118, 107.3244,\n",
       "         87.4455,  77.9896,      nan, 165.9587,      nan,      nan,  91.6218,\n",
       "        106.5444,  77.8209,  92.7899,  88.1274,  83.0310, 124.4268, 117.2981,\n",
       "         72.2753,  85.4627,  72.3388,      nan,  88.5128,  97.4613,  74.8860,\n",
       "        107.8487, 125.9268,      nan,      nan,  99.1933,  72.3024,      nan,\n",
       "         70.3280, 110.3659,      nan,      nan, 113.4713,      nan,  51.2862,\n",
       "        167.8261, 143.9467,  54.1493,      nan,      nan, 148.6958,      nan,\n",
       "        103.5806,  55.2882, 104.5135, 121.2661,      nan, 121.4896,  90.1288,\n",
       "         83.5073,  84.0880, 127.4907,  74.9793,  88.6567, 110.6601,      nan,\n",
       "         83.2907,      nan,      nan,  77.0816,      nan,      nan, 107.2345,\n",
       "        111.9074,      nan,  99.9308,      nan,      nan,  95.6329, 124.7325,\n",
       "             nan,  24.4192,      nan,  77.3125,  78.5033,  63.4018,      nan,\n",
       "        118.1039,      nan,  75.9435,  37.4501,  75.2026,      nan,  88.5910,\n",
       "        105.5159,      nan,      nan, 141.8752,  93.3719,  92.2034,      nan,\n",
       "             nan,      nan,  58.9634,  62.1196,  64.4081, 117.9515, 144.9805,\n",
       "         91.1001,  80.0595, 113.0848,  88.9851,  53.0857,  92.6858,  20.5723,\n",
       "         92.2359, 139.6687, 111.5324, 100.0644, 110.8792, 118.4521, 100.5292,\n",
       "        121.0642,  66.3683,      nan, 115.9049,  89.2415,      nan,  49.6847,\n",
       "        127.3971, 149.7113, 115.5046,  93.3203,  70.6737,      nan,  97.9289,\n",
       "         60.6877, 117.2462,  79.2493,      nan, 114.5526,  96.2946, 139.0817,\n",
       "             nan,      nan,  90.0202,      nan,  87.6439,      nan,  87.5021,\n",
       "         99.1355,  90.1252, 115.5032,  98.4823, 146.2283, 101.9667,      nan,\n",
       "        104.3264,  46.0107,      nan,  90.2583, 109.6399,  71.9604, 105.9838,\n",
       "         47.5826, 129.3486,  69.8072,  88.1896, 111.1318,  87.9035, 110.0506,\n",
       "        119.1806,  96.5107,  82.5551,      nan,  68.0800, 105.3565,      nan,\n",
       "         67.9688, 123.2653, 114.2300,      nan,      nan,      nan, 112.7799,\n",
       "         73.2159,  94.2482,      nan,  80.3556,      nan,      nan,  95.2565,\n",
       "        118.1542,      nan, 118.8312, 120.7875, 122.8418,  86.9834, 110.3025,\n",
       "        101.4314,      nan,      nan,  75.7931,  66.7888, 109.5332,      nan,\n",
       "        108.3668,      nan, 126.3276,      nan, 109.1689,      nan, 100.7707,\n",
       "             nan, 111.6617,      nan,  73.5891,  89.3727,  55.7794, 122.7195,\n",
       "        175.3730, 107.7433, 103.7113,  97.6241,  53.2465,  96.5639,      nan,\n",
       "         83.2289,  32.9244,      nan,  82.3511,      nan, 125.3128, 111.6666,\n",
       "         87.2279, 113.4698,      nan, 113.0070,   7.3506, 120.6135,  64.3739,\n",
       "         49.3162, 109.0101, 131.7720, 128.0522,  76.7048,  85.8376, 111.7794,\n",
       "         81.9836, 125.5314, 118.9672,  88.8102, 115.2718, 116.1375,  76.6449,\n",
       "             nan,      nan, 104.6886,      nan,  89.5036, 121.0477,  84.3409,\n",
       "        135.3965, 113.4442,  61.8550, 103.6368, 158.1470, 151.2556,  42.0717,\n",
       "             nan,  36.4468,  86.1755,  66.3864,  65.4462,  79.6089, 148.5954,\n",
       "             nan,  30.5933,  78.5036,  29.8966,  89.9938, 101.0699,      nan,\n",
       "         52.1469,  80.4802, 124.2722, 122.1730,  73.8105,      nan,  50.7099,\n",
       "        102.9899, 109.7444, 102.7422,      nan,      nan,  59.3643,  95.5123,\n",
       "         88.9002,      nan, 139.6607,      nan, 117.8536,  71.8290, 127.1596,\n",
       "        113.2097,      nan,  74.9691,      nan, 141.1892, 108.5325, 123.8534,\n",
       "         85.1197,  81.2071, 100.7576,      nan,  92.9853,      nan,  87.7108,\n",
       "         67.1642,  45.3414,  79.4523,  67.8104, 102.0128,  96.2227,  81.1567,\n",
       "         71.6511, 118.3189,  50.9134, 123.6606,  57.9009, 120.3032,  60.3108,\n",
       "             nan, 105.2718,      nan,      nan,      nan,  70.8131,      nan,\n",
       "         52.6152, 135.0229,  98.7987,      nan,      nan, 134.3888,  52.4178,\n",
       "             nan,      nan, 126.7616, 110.2486,  83.0955,  83.2492, 109.5320,\n",
       "         84.0424,      nan,  83.7767,  74.7009, 127.8922,  84.0046, 121.4943,\n",
       "         84.0191,  97.0443,  60.7224,      nan,  88.9737,  76.4334,  87.1741,\n",
       "         87.5069,      nan,  82.8441,      nan,      nan, 118.1354,  95.9496,\n",
       "        140.1734, 117.8152,  58.3471,      nan,      nan,      nan,  74.2684,\n",
       "             nan, 134.0518, 118.0157,      nan,  72.3469,  57.1922,      nan,\n",
       "         73.8097, 132.4630, 154.4309, 106.3986,      nan, 106.8275, 147.9929,\n",
       "             nan, 100.4229,      nan, 110.2180,  57.0309, 116.9936,      nan,\n",
       "         93.8899,  99.8954, 102.8675, 100.5781, 144.6571, 116.6574,      nan,\n",
       "         34.3516, 109.8365, 120.0660, 107.2630, 130.0544,      nan,  84.1250,\n",
       "        107.6488,      nan,      nan, 115.0903,      nan,  63.7110, 118.6645,\n",
       "        127.7741, 100.5855,      nan,  86.3434, 114.5138,  42.2286,  96.5354,\n",
       "             nan, 149.8026, 105.1621, 148.8358,      nan,  90.5999, 118.7168,\n",
       "        137.7864,  65.3164,  77.4498, 138.4663, 115.1409,  52.0071, 110.7374,\n",
       "        120.2330, 133.4001,      nan,  23.9819,  88.8289,      nan, 117.3446,\n",
       "             nan, 139.8732,      nan,  88.7533,      nan,  81.7549, 106.4209,\n",
       "         91.4101,      nan, 100.2473,  70.1835, 118.8214,      nan,  62.5135,\n",
       "        127.4637,      nan, 117.6862,  89.2029,  64.8250, 108.7402,      nan,\n",
       "         95.9878,      nan,      nan, 110.0648,  84.1266, 103.0213,      nan,\n",
       "         95.8479,  85.3262,      nan, 154.0188,  74.1074,  69.3835,  76.0127,\n",
       "         95.6524,      nan, 110.3606,  89.0703,      nan,  98.6994,  89.4353,\n",
       "         75.1492,      nan,  59.6931,      nan, 131.7071,      nan,  73.1883,\n",
       "        123.7217,      nan,  69.7120,  37.1165,      nan, 109.0829, 137.1466,\n",
       "         60.2038,  93.9032,  55.9973, 133.0456,  68.7934,  77.6950,  89.6682,\n",
       "        157.9635, 104.6343, 118.4255,  65.6509,  88.6207,      nan,  58.2576,\n",
       "         57.1789,  71.9472,      nan,  94.7994,      nan,  87.4379,      nan,\n",
       "         90.2054, 136.2289,  86.0752, 121.5504, 118.2537, 112.9257, 135.6770,\n",
       "        109.8409, 115.9178,  81.3897,  54.9598, 106.6397,  63.1417,  82.9692,\n",
       "         86.9428,  65.7301, 118.8369, 123.7149,      nan,  69.7123, 104.5150,\n",
       "             nan,  42.1887,  96.2642, 104.5760,  77.0331,      nan,      nan,\n",
       "         52.8099,      nan, 119.2203,      nan,  88.1370,  95.2033, 110.3481,\n",
       "        148.5394,  92.9421, 130.9605, 103.0407, 115.3350, 107.5206, 166.4034,\n",
       "         93.6057, 105.1052,  62.9633, 114.9125, 141.6061,  79.9670,  47.4154,\n",
       "         98.9128,  77.2057,      nan, 108.8724,  82.6886,      nan,  71.3524,\n",
       "         67.8243, 120.4107,  88.5717,  82.1954,      nan,  45.1657,      nan,\n",
       "        131.3199,  89.6272, 119.5788,  73.2565,  67.9942, 103.3442, 111.8834,\n",
       "         90.5407, 130.0489, 118.4570,  45.6271,  87.0977,  67.0192, 119.3069,\n",
       "        123.1944,  57.8063,  96.9605,      nan,  46.2370,      nan,  60.2165,\n",
       "         78.5206, 141.4197, 100.4722, 114.6037,  85.2915,      nan,  54.6166,\n",
       "        122.3485,  88.0571,      nan, 110.7097, 116.0142,  57.8267,  53.7676,\n",
       "        117.4530,  25.1764,      nan, 110.0414, 126.4517,  96.0529,  78.6308,\n",
       "         94.7385,  70.1737, 125.4627,      nan,  86.5776,  78.7067,  90.6604,\n",
       "             nan, 123.4252,  90.9034, 126.3126,  61.7786, 114.8335,  56.7209,\n",
       "             nan,      nan, 111.7103,      nan, 135.0796, 102.0275,      nan,\n",
       "        137.1956,  74.9756, 100.7490,      nan, 165.7574, 108.3888, 120.8636,\n",
       "         30.6735, 105.4421,  62.8827,  91.3181,  94.2419,  64.6659, 168.3975,\n",
       "        110.2007,  82.4117,      nan,  44.3114,      nan,  16.7858,  82.4054,\n",
       "         92.2963,      nan,  72.9333, 122.6984,  69.4217,  92.1501,  50.1506,\n",
       "             nan, 101.3102,  75.1620, 110.8940,  55.1147,  76.5139, 106.1123,\n",
       "         47.3360,  78.8627,  93.1168,  70.8160, 110.2723,  47.6232, 118.7798,\n",
       "        109.3723,      nan, 117.5987, 114.9239,  98.3051, 142.2986,  87.7973,\n",
       "        116.1720, 104.9899,  94.0372, 113.5712, 120.5207,  74.3860,      nan,\n",
       "        102.6084,  67.7942,      nan,  79.4309,  61.7359,  44.8933, 117.3319,\n",
       "         86.8858,      nan,  54.7581,  91.1262,      nan,  88.6429,      nan,\n",
       "        106.7586, 117.5221,  62.0049, 113.2865,      nan,  48.3576,      nan,\n",
       "        103.0808,      nan,  98.7281, 119.0177, 111.1123, 117.6160,  95.1893,\n",
       "             nan,  77.1174, 109.9773,  97.7716,      nan,  72.0244, 100.0670,\n",
       "         76.3496,      nan,  66.7260,  34.5595,      nan,  98.8823,  52.7949,\n",
       "             nan,  99.4549,      nan,      nan, 103.1021, 108.4390, 108.1796,\n",
       "         70.5518,  25.9905,  42.0425,      nan,      nan, 107.0881,      nan,\n",
       "         78.6876, 117.7606,      nan,      nan,  82.4279,      nan,  55.3337,\n",
       "        118.4517,      nan, 106.7772,  85.9316, 122.8778,      nan, 111.6115,\n",
       "             nan,  92.1884,  94.4775,  63.2873,  91.5353,  55.3460, 168.5630,\n",
       "         67.8092,  75.5331, 123.6293,  88.6310,      nan,      nan])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dif_rotacion*180/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     nan,      nan, 113.7581,  67.7534,  87.9574,      nan,  96.5356,\n",
       "         49.6054,  91.6569, 119.5670,  93.1966, 114.7472,      nan,  91.2317,\n",
       "             nan, 128.5368, 116.9089,      nan,  99.7445, 128.0478,      nan,\n",
       "             nan,  73.1933,  55.1062,  64.6789, 105.1331, 119.3066, 123.4233,\n",
       "        113.7058,      nan, 114.2222, 105.9027,  59.7415,      nan,  96.5179,\n",
       "             nan,  76.0669,  70.7353, 158.6364, 110.2613, 132.4500, 156.9274,\n",
       "        121.4257, 106.1061,  75.8529, 120.4056,      nan, 118.7304,  55.5627,\n",
       "         68.3648, 112.0424,  62.8928, 116.2398,      nan,      nan, 104.0471,\n",
       "         80.9113, 118.7672,  87.0493,  68.2372,  90.8746,  70.5836,      nan,\n",
       "        112.7893, 159.5592, 119.0011, 103.9122,  98.6167, 108.4417,  61.0209,\n",
       "             nan,  57.3849,  62.7486,  93.5651,      nan, 103.0579, 144.1972,\n",
       "             nan,  68.6963, 107.8300, 107.4953, 153.0553,  97.1018,  99.5971,\n",
       "         82.2315, 115.6954,  98.8359,      nan, 112.1337,  64.7071, 101.3380,\n",
       "        169.4515, 103.5692,  71.4726, 113.9387,      nan, 104.7065,      nan,\n",
       "             nan,      nan,      nan,      nan,  55.8073, 116.3809,  85.7880,\n",
       "          8.6086,  24.5910, 109.8402,  39.2612,  95.0550,  94.4179, 119.1908,\n",
       "             nan,  87.8172,  74.1126,  68.4074,      nan,  95.9254,  86.2631,\n",
       "         75.1235, 108.6993, 141.6164,  62.4481, 110.8046, 137.1870, 173.3909,\n",
       "         61.8015,  61.8300,      nan, 118.2447,  74.7556, 119.5843,  81.6933,\n",
       "             nan,      nan, 120.5509,      nan, 112.9242,      nan, 122.6777,\n",
       "         30.1186, 121.5763,      nan,  81.9452,      nan,  35.3798,  50.5125,\n",
       "         76.3586,  72.0967, 108.0260, 112.4800, 123.0924, 106.9562,  88.1499,\n",
       "             nan,  88.9910,  60.3180, 101.7194, 122.2137,  52.0063,  53.8066,\n",
       "         60.8248,      nan,  79.2981,      nan,  56.3994,  71.8198,  89.2720,\n",
       "        106.4483,      nan,  70.8562,      nan,  98.1543,  60.5712, 101.6777,\n",
       "        154.4105,      nan,  97.0637,      nan,  38.5222,  68.1112,  98.2943,\n",
       "             nan, 104.7397,      nan, 103.9964,  74.7642, 110.5784,  67.3057,\n",
       "         73.3821, 114.0764,  73.8988,  63.3433,  56.0125, 107.6682,      nan,\n",
       "             nan,  81.0382, 127.2897,      nan,      nan, 109.1073,      nan,\n",
       "         68.4818, 152.1267,  86.1041,  90.1713,      nan, 140.9563,  60.5743,\n",
       "             nan,      nan,  88.7662,  70.2880,  75.6497,  94.3118, 107.3244,\n",
       "         87.4455,  77.9896,      nan, 165.9587,      nan,      nan,  91.6218,\n",
       "        106.5444,  77.8209,  92.7899,  88.1274,  83.0310, 124.4268, 117.2981,\n",
       "         72.2753,  85.4627,  72.3388,      nan,  88.5128,  97.4613,  74.8860,\n",
       "        107.8487, 125.9268,      nan,      nan,  99.1933,  72.3024,      nan,\n",
       "         70.3280, 110.3659,      nan,      nan, 113.4713,      nan,  51.2862,\n",
       "        167.8261, 143.9467,  54.1493,      nan,      nan, 148.6958,      nan,\n",
       "        103.5806,  55.2882, 104.5135, 121.2661,      nan, 121.4896,  90.1288,\n",
       "         83.5073,  84.0880, 127.4907,  74.9793,  88.6567, 110.6601,      nan,\n",
       "         83.2907,      nan,      nan,  77.0815,      nan,      nan, 107.2345,\n",
       "        111.9074,      nan,  99.9308,      nan,      nan,  95.6329, 124.7325,\n",
       "             nan,  24.4193,      nan,  77.3125,  78.5033,  63.4018,      nan,\n",
       "        118.1039,      nan,  75.9435,  37.4501,  75.2026,      nan,  88.5910,\n",
       "        105.5159,      nan,      nan, 141.8752,  93.3719,  92.2034,      nan,\n",
       "             nan,      nan,  58.9634,  62.1196,  64.4081, 117.9515, 144.9805,\n",
       "         91.1001,  80.0595, 113.0848,  88.9851,  53.0857,  92.6858,  20.5724,\n",
       "         92.2359, 139.6687, 111.5324, 100.0644, 110.8792, 118.4521, 100.5292,\n",
       "        121.0642,  66.3683,      nan, 115.9049,  89.2415,      nan,  49.6847,\n",
       "        127.3971, 149.7113, 115.5046,  93.3203,  70.6737,      nan,  97.9289,\n",
       "         60.6877, 117.2462,  79.2493,      nan, 114.5526,  96.2946, 139.0817,\n",
       "             nan,      nan,  90.0202,      nan,  87.6439,      nan,  87.5021,\n",
       "         99.1355,  90.1252, 115.5032,  98.4823, 146.2283, 101.9667,      nan,\n",
       "        104.3264,  46.0107,      nan,  90.2583, 109.6399,  71.9604, 105.9838,\n",
       "         47.5827, 129.3485,  69.8072,  88.1896, 111.1318,  87.9035, 110.0506,\n",
       "        119.1806,  96.5107,  82.5551,      nan,  68.0800, 105.3565,      nan,\n",
       "         67.9688, 123.2653, 114.2300,      nan,      nan,      nan, 112.7799,\n",
       "         73.2159,  94.2482,      nan,  80.3556,      nan,      nan,  95.2565,\n",
       "        118.1542,      nan, 118.8312, 120.7875, 122.8418,  86.9834, 110.3025,\n",
       "        101.4314,      nan,      nan,  75.7931,  66.7888, 109.5332,      nan,\n",
       "        108.3668,      nan, 126.3276,      nan, 109.1689,      nan, 100.7707,\n",
       "             nan, 111.6617,      nan,  73.5891,  89.3727,  55.7794, 122.7195,\n",
       "        175.3730, 107.7433, 103.7113,  97.6241,  53.2465,  96.5639,      nan,\n",
       "         83.2289,  32.9244,      nan,  82.3511,      nan, 125.3128, 111.6666,\n",
       "         87.2279, 113.4698,      nan, 113.0070,   7.3507, 120.6135,  64.3739,\n",
       "         49.3162, 109.0101, 131.7720, 128.0522,  76.7048,  85.8376, 111.7794,\n",
       "         81.9836, 125.5314, 118.9672,  88.8102, 115.2718, 116.1375,  76.6449,\n",
       "             nan,      nan, 104.6886,      nan,  89.5036, 121.0477,  84.3409,\n",
       "        135.3965, 113.4442,  61.8550, 103.6368, 158.1469, 151.2556,  42.0717,\n",
       "             nan,  36.4468,  86.1755,  66.3864,  65.4462,  79.6089, 148.5955,\n",
       "             nan,  30.5933,  78.5036,  29.8966,  89.9938, 101.0699,      nan,\n",
       "         52.1469,  80.4802, 124.2722, 122.1730,  73.8105,      nan,  50.7099,\n",
       "        102.9899, 109.7444, 102.7422,      nan,      nan,  59.3643,  95.5123,\n",
       "         88.9002,      nan, 139.6607,      nan, 117.8536,  71.8290, 127.1596,\n",
       "        113.2097,      nan,  74.9691,      nan, 141.1892, 108.5325, 123.8534,\n",
       "         85.1197,  81.2071, 100.7576,      nan,  92.9853,      nan,  87.7108,\n",
       "         67.1642,  45.3414,  79.4523,  67.8104, 102.0128,  96.2227,  81.1567,\n",
       "         71.6511, 118.3189,  50.9134, 123.6606,  57.9009, 120.3032,  60.3108,\n",
       "             nan, 105.2718,      nan,      nan,      nan,  70.8131,      nan,\n",
       "         52.6152, 135.0229,  98.7987,      nan,      nan, 134.3888,  52.4178,\n",
       "             nan,      nan, 126.7616, 110.2486,  83.0955,  83.2492, 109.5320,\n",
       "         84.0425,      nan,  83.7767,  74.7009, 127.8922,  84.0046, 121.4943,\n",
       "         84.0191,  97.0443,  60.7224,      nan,  88.9737,  76.4334,  87.1741,\n",
       "         87.5069,      nan,  82.8441,      nan,      nan, 118.1354,  95.9496,\n",
       "        140.1734, 117.8152,  58.3471,      nan,      nan,      nan,  74.2684,\n",
       "             nan, 134.0518, 118.0157,      nan,  72.3469,  57.1922,      nan,\n",
       "         73.8097, 132.4630, 154.4309, 106.3986,      nan, 106.8275, 147.9929,\n",
       "             nan, 100.4229,      nan, 110.2180,  57.0309, 116.9936,      nan,\n",
       "         93.8899,  99.8954, 102.8675, 100.5781, 144.6571, 116.6574,      nan,\n",
       "         34.3516, 109.8365, 120.0661, 107.2630, 130.0544,      nan,  84.1250,\n",
       "        107.6488,      nan,      nan, 115.0903,      nan,  63.7110, 118.6645,\n",
       "        127.7741, 100.5855,      nan,  86.3434, 114.5138,  42.2285,  96.5354,\n",
       "             nan, 149.8026, 105.1621, 148.8358,      nan,  90.5999, 118.7168,\n",
       "        137.7864,  65.3164,  77.4498, 138.4663, 115.1409,  52.0071, 110.7374,\n",
       "        120.2330, 133.4001,      nan,  23.9819,  88.8289,      nan, 117.3446,\n",
       "             nan, 139.8732,      nan,  88.7533,      nan,  81.7549, 106.4209,\n",
       "         91.4101,      nan, 100.2473,  70.1835, 118.8214,      nan,  62.5135,\n",
       "        127.4637,      nan, 117.6862,  89.2029,  64.8250, 108.7402,      nan,\n",
       "         95.9878,      nan,      nan, 110.0648,  84.1266, 103.0213,      nan,\n",
       "         95.8479,  85.3262,      nan, 154.0188,  74.1073,  69.3835,  76.0127,\n",
       "         95.6524,      nan, 110.3606,  89.0703,      nan,  98.6994,  89.4353,\n",
       "         75.1492,      nan,  59.6931,      nan, 131.7071,      nan,  73.1883,\n",
       "        123.7217,      nan,  69.7120,  37.1165,      nan, 109.0829, 137.1466,\n",
       "         60.2038,  93.9032,  55.9973, 133.0457,  68.7934,  77.6950,  89.6682,\n",
       "        157.9635, 104.6343, 118.4255,  65.6509,  88.6207,      nan,  58.2576,\n",
       "         57.1789,  71.9472,      nan,  94.7993,      nan,  87.4379,      nan,\n",
       "         90.2054, 136.2289,  86.0752, 121.5504, 118.2537, 112.9257, 135.6770,\n",
       "        109.8409, 115.9178,  81.3897,  54.9598, 106.6397,  63.1417,  82.9692,\n",
       "         86.9428,  65.7301, 118.8369, 123.7149,      nan,  69.7123, 104.5150,\n",
       "             nan,  42.1887,  96.2642, 104.5760,  77.0331,      nan,      nan,\n",
       "         52.8099,      nan, 119.2203,      nan,  88.1370,  95.2033, 110.3481,\n",
       "        148.5394,  92.9421, 130.9605, 103.0407, 115.3350, 107.5206, 166.4034,\n",
       "         93.6058, 105.1052,  62.9633, 114.9125, 141.6061,  79.9670,  47.4154,\n",
       "         98.9128,  77.2057,      nan, 108.8724,  82.6886,      nan,  71.3524,\n",
       "         67.8243, 120.4107,  88.5716,  82.1954,      nan,  45.1657,      nan,\n",
       "        131.3198,  89.6272, 119.5788,  73.2565,  67.9942, 103.3442, 111.8834,\n",
       "         90.5407, 130.0489, 118.4570,  45.6271,  87.0977,  67.0192, 119.3069,\n",
       "        123.1944,  57.8063,  96.9605,      nan,  46.2370,      nan,  60.2165,\n",
       "         78.5206, 141.4197, 100.4722, 114.6037,  85.2915,      nan,  54.6166,\n",
       "        122.3485,  88.0571,      nan, 110.7097, 116.0142,  57.8267,  53.7676,\n",
       "        117.4530,  25.1764,      nan, 110.0414, 126.4517,  96.0529,  78.6308,\n",
       "         94.7385,  70.1737, 125.4627,      nan,  86.5776,  78.7067,  90.6604,\n",
       "             nan, 123.4252,  90.9034, 126.3126,  61.7786, 114.8335,  56.7210,\n",
       "             nan,      nan, 111.7103,      nan, 135.0796, 102.0275,      nan,\n",
       "        137.1956,  74.9756, 100.7490,      nan, 165.7574, 108.3888, 120.8636,\n",
       "         30.6735, 105.4421,  62.8827,  91.3181,  94.2419,  64.6659, 168.3975,\n",
       "        110.2007,  82.4117,      nan,  44.3114,      nan,  16.7858,  82.4054,\n",
       "         92.2963,      nan,  72.9333, 122.6984,  69.4217,  92.1501,  50.1506,\n",
       "             nan, 101.3102,  75.1620, 110.8940,  55.1147,  76.5139, 106.1123,\n",
       "         47.3360,  78.8627,  93.1168,  70.8160, 110.2723,  47.6232, 118.7798,\n",
       "        109.3724,      nan, 117.5987, 114.9239,  98.3051, 142.2986,  87.7973,\n",
       "        116.1720, 104.9899,  94.0372, 113.5712, 120.5207,  74.3860,      nan,\n",
       "        102.6084,  67.7941,      nan,  79.4309,  61.7359,  44.8933, 117.3319,\n",
       "         86.8858,      nan,  54.7581,  91.1262,      nan,  88.6429,      nan,\n",
       "        106.7586, 117.5221,  62.0049, 113.2865,      nan,  48.3576,      nan,\n",
       "        103.0808,      nan,  98.7281, 119.0177, 111.1123, 117.6160,  95.1893,\n",
       "             nan,  77.1174, 109.9773,  97.7716,      nan,  72.0244, 100.0670,\n",
       "         76.3496,      nan,  66.7260,  34.5595,      nan,  98.8823,  52.7949,\n",
       "             nan,  99.4549,      nan,      nan, 103.1021, 108.4390, 108.1796,\n",
       "         70.5518,  25.9905,  42.0425,      nan,      nan, 107.0881,      nan,\n",
       "         78.6876, 117.7606,      nan,      nan,  82.4279,      nan,  55.3337,\n",
       "        118.4517,      nan, 106.7772,  85.9316, 122.8778,      nan, 111.6115,\n",
       "             nan,  92.1884,  94.4775,  63.2873,  91.5353,  55.3460, 168.5630,\n",
       "         67.8092,  75.5331, 123.6293,  88.6310,      nan,      nan])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dif_rotacion*180/np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
